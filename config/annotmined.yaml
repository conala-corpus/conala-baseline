# This is a configuration file for xnmt (https://github.com/neulab/xnmt) that
# can be used to train a sequence-to-sequence model baseline for the Conala
# shared task (https://conala-corpus.github.io). It uses the small annotated
# training set and the large mined noisy data.
#
# Tokens are split into subtokens (vocabulary size 16000) using sentencepiece,
# and the baseline is a standard seq2seq model.
annotmined: !Experiment
  exp_global: !ExpGlobal
    default_layer_dim: 512
    dropout: 0.3
    log_file: 'results/{EXP}.log'
    model_file: 'results/{EXP}.mod'
  preproc: !PreprocRunner
    overwrite: False
    tasks:
    - !PreprocTokenize
      in_files:
      - 'conala-corpus/conala-trainnodev+mined.intent'
      - 'conala-corpus/conala-trainnodev+mined.snippet'
      - 'conala-corpus/conala-dev.intent'
      - 'conala-corpus/conala-dev.snippet'
      - 'conala-corpus/conala-test.intent'
      - 'conala-corpus/conala-test.snippet'
      out_files:
      - 'conala-corpus/conala-trainnodev+mined.tmspm16000.intent'
      - 'conala-corpus/conala-trainnodev+mined.tmspm16000.snippet'
      - 'conala-corpus/conala-dev.tmspm16000.intent'
      - 'conala-corpus/conala-dev.tmspm16000.snippet'
      - 'conala-corpus/conala-test.tmspm16000.intent'
      - 'conala-corpus/conala-test.tmspm16000.snippet'
      specs:
      - filenum: all
        tokenizers:
        - !SentencepieceTokenizer
          path: ''
          train_files:
          - 'conala-corpus/conala-trainnodev+mined.intent'
          - 'conala-corpus/conala-trainnodev+mined.snippet'
          vocab_size: 16000
          model_type: unigram
          model_prefix: 'conala-corpus/conala-trainnodev+mined.tmspm16000.spm'
    - !PreprocVocab
      in_files:
      - 'conala-corpus/conala-trainnodev+mined.tmspm16000.intent'
      - 'conala-corpus/conala-trainnodev+mined.tmspm16000.snippet'
      out_files:
      - 'conala-corpus/conala-trainnodev+mined.tmspm16000.intent.vocab'
      - 'conala-corpus/conala-trainnodev+mined.tmspm16000.snippet.vocab'
      specs:
      - filenum: all
        spec:
        - type: freq
          min_freq: 2
  model: !DefaultTranslator
    src_reader: !PlainTextReader
      vocab: !Vocab {vocab_file: 'conala-corpus/conala-trainnodev+mined.tmspm16000.intent.vocab'}
    trg_reader: !PlainTextReader
      vocab: !Vocab {vocab_file: 'conala-corpus/conala-trainnodev+mined.tmspm16000.snippet.vocab'}
    src_embedder: !SimpleWordEmbedder
      emb_dim: 128
    encoder: !BiLSTMSeqTransducer
      layers: 1
    attender: !MlpAttender {}
    trg_embedder: !SimpleWordEmbedder
      emb_dim: 128
    decoder: !MlpSoftmaxDecoder
      bridge: !CopyBridge {}
    inference: !SimpleInference
      search_strategy: !BeamSearch
        len_norm: !PolynomialNormalization
          apply_during_search: True
        beam_size: 5
      post_process: join-piece
  train: !SimpleTrainingRegimen
    trainer: !AdamTrainer
      alpha: 0.001
    batcher: !WordSrcBatcher
      avg_batch_size: 64
    patience: 3
    lr_decay: 0.5
    restart_trainer: True
    run_for_epochs: 30
    src_file: 'conala-corpus/conala-trainnodev+mined.tmspm16000.intent'
    trg_file: 'conala-corpus/conala-trainnodev+mined.tmspm16000.snippet'
    dev_tasks:
      - !AccuracyEvalTask
        eval_metrics: bleu
        src_file: 'conala-corpus/conala-dev.tmspm16000.intent'
        ref_file: 'conala-corpus/conala-dev.snippet'
        hyp_file: 'results/{EXP}.dev.hyp'
      - !LossEvalTask
        src_file: 'conala-corpus/conala-dev.tmspm16000.intent'
        ref_file: 'conala-corpus/conala-dev.tmspm16000.snippet'
  evaluate:
    - !AccuracyEvalTask
      eval_metrics: bleu
      src_file: 'conala-corpus/conala-test.tmspm16000.intent'
      ref_file: 'conala-corpus/conala-test.snippet'
      hyp_file: 'results/{EXP}.test.hyp'

